{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task 6: Music Genre Classification (GTZAN Dataset)","metadata":{}},{"cell_type":"markdown","source":"\n**1) Tabular approach (MFCC + traditional ML)**\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Set dataset path\nDATASET_PATH = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original\"\nSPECTRO_PATH = \"spectrograms\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Extract better MFCC features for tabular model\ndef extract_features_tabular(dataset_path):\n    features = []\n    labels = []\n    for genre in os.listdir(dataset_path):\n        genre_path = os.path.join(dataset_path, genre)\n        if not os.path.isdir(genre_path):\n            continue\n        for file in os.listdir(genre_path):\n            file_path = os.path.join(genre_path, file)\n            try:\n                y, sr = librosa.load(file_path, duration=30)\n                \n                # Core MFCCs\n                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n                mfcc_mean = np.mean(mfcc.T, axis=0)\n                \n                # Extra features for more info\n                chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n                chroma_mean = np.mean(chroma.T, axis=0)\n                \n                contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n                contrast_mean = np.mean(contrast.T, axis=0)\n                \n                tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n                tonnetz_mean = np.mean(tonnetz.T, axis=0)\n                \n                # Combine all\n                feature_vector = np.hstack([mfcc_mean, chroma_mean, contrast_mean, tonnetz_mean])\n                features.append(feature_vector)\n                labels.append(genre)\n            except Exception as e:\n                print(f\"Error processing {file_path}: {e}\")\n    return np.array(features), np.array(labels)\n\nprint(\"Extracting MFCC + extra features for RandomForest...\")\nX_tabular, y_tabular = extract_features_tabular(DATASET_PATH)\n\n# Encode labels\nencoder = LabelEncoder()\ny_tabular_encoded = encoder.fit_transform(y_tabular)\n\n# Scale\nscaler = StandardScaler()\nX_tabular_scaled = scaler.fit_transform(X_tabular)\n\n# Train/test split\nX_train_tab, X_test_tab, y_train_tab, y_test_tab = train_test_split(\n    X_tabular_scaled, y_tabular_encoded, test_size=0.2, random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: RandomForest with tuning\nprint(\"\\nTraining tuned RandomForest...\")\nrf = RandomForestClassifier(random_state=42)\nparam_grid = {\n    'n_estimators': [200, 300],\n    'max_depth': [None, 20, 30],\n    'min_samples_split': [2, 4],\n}\ngrid_search = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1, verbose=1)\ngrid_search.fit(X_train_tab, y_train_tab)\nbest_rf = grid_search.best_estimator_\n\ny_pred_tab = best_rf.predict(X_test_tab)\nprint(\"\\n--- RandomForest Results ---\")\nprint(\"Accuracy:\", accuracy_score(y_test_tab, y_pred_tab))\nprint(classification_report(y_test_tab, y_pred_tab, target_names=encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:16:25.292743Z","iopub.execute_input":"2025-08-14T16:16:25.293297Z","iopub.status.idle":"2025-08-14T16:16:38.759318Z","shell.execute_reply.started":"2025-08-14T16:16:25.293273Z","shell.execute_reply":"2025-08-14T16:16:38.758514Z"}},"outputs":[{"name":"stdout","text":"\nTraining tuned RandomForest...\nFitting 3 folds for each of 12 candidates, totalling 36 fits\n\n--- RandomForest Results ---\nAccuracy: 0.71\n              precision    recall  f1-score   support\n\n       blues       0.73      0.73      0.73        22\n   classical       0.90      0.93      0.91        28\n     country       0.67      0.73      0.70        22\n       disco       0.62      0.62      0.62        21\n      hiphop       0.64      0.74      0.68        19\n        jazz       0.71      0.71      0.71        17\n       metal       0.80      1.00      0.89        12\n         pop       0.75      0.75      0.75        20\n      reggae       0.58      0.58      0.58        24\n        rock       0.67      0.27      0.38        15\n\n    accuracy                           0.71       200\n   macro avg       0.71      0.70      0.69       200\nweighted avg       0.71      0.71      0.70       200\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"**2) Image-based approach (Spectrograms Transfer Learning)**","metadata":{}},{"cell_type":"code","source":"# Step 4: Create spectrogram images\ndef create_clean_spectrograms(dataset_path, output_path, img_size=(128, 128)):\n    os.makedirs(output_path, exist_ok=True)\n    for genre in os.listdir(dataset_path):\n        genre_path = os.path.join(dataset_path, genre)\n        if not os.path.isdir(genre_path):\n            continue\n\n        genre_out_path = os.path.join(output_path, genre)\n        os.makedirs(genre_out_path, exist_ok=True)\n\n        for file in os.listdir(genre_path):\n            if not file.lower().endswith(\".wav\"):\n                continue\n\n            file_path = os.path.join(genre_path, file)\n            try:\n                # Load first 10 seconds for consistency\n                y, sr = librosa.load(file_path, duration=10)\n\n                # Compute mel spectrogram\n                melspec = librosa.feature.melspectrogram(\n                    y=y, sr=sr, n_mels=128, fmax=8000\n                )\n                melspec_db = librosa.power_to_db(melspec, ref=np.max)\n\n                # Create plot without axes\n                fig = plt.figure(figsize=(img_size[0] / 100, img_size[1] / 100), dpi=100)\n                ax = plt.Axes(fig, [0., 0., 1., 1.])\n                ax.set_axis_off()\n                fig.add_axes(ax)\n\n                librosa.display.specshow(\n                    melspec_db,\n                    sr=sr,\n                    cmap=\"magma\",\n                    vmin=-80, vmax=0  # fixed color scale\n                )\n\n                save_path = os.path.join(genre_out_path, file.replace(\".wav\", \".png\"))\n                plt.savefig(save_path, bbox_inches=None, pad_inches=0)\n                plt.close(fig)\n\n            except Exception as e:\n                print(f\"Error processing {file_path}: {e}\")\n\nprint(\"\\nCreating clean spectrograms...\")\ncreate_clean_spectrograms(DATASET_PATH, SPECTRO_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Image generators with augmentation\nIMG_SIZE = (128, 128)\nBATCH_SIZE = 32\n\ntrain_gen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n)\n\ntrain_data = train_gen.flow_from_directory(\n    SPECTRO_PATH,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    subset=\"training\"\n)\nval_data = train_gen.flow_from_directory(\n    SPECTRO_PATH,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    subset=\"validation\"\n)\n\nprint(\"\\nTraining Transfer Learning model (VGG16)...\")\n\n# Load base model without top layers\nbase_model = VGG16(weights='imagenet', include_top=False,\n                   input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n\n# Unfreeze last 8 layers for fine-tuning\nfor layer in base_model.layers[:-8]:\n    layer.trainable = False\nfor layer in base_model.layers[-8:]:\n    layer.trainable = True\n\n# Build transfer learning model\ntransfer_model = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(len(train_data.class_indices), activation='softmax')\n])\n\n# Compile with Adam + weight decay for stability\ntransfer_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\ncallbacks = [\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n]\n\n# Train model\nhistory = transfer_model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=50,\n    callbacks=callbacks\n)\n\n# Step 7:compare results\n\n# Tabular model accuracy\ntabular_acc = accuracy_score(y_test_tab, y_pred_tab)\n\n# Transfer Learning accuracy\nvgg_loss, vgg_acc = transfer_model.evaluate(val_data, verbose=0)\n\n# Create comparison table\nresults_df = pd.DataFrame({\n    \"Model\": [\"RandomForest (MFCC+Extra Features)\", \"VGG16 Transfer Learning\"],\n    \"Accuracy\": [tabular_acc, vgg_acc]\n})\n\nprint(\"\\n--- Model Accuracy Comparison ---\")\nprint(results_df.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T17:07:55.062078Z","iopub.execute_input":"2025-08-14T17:07:55.062382Z","iopub.status.idle":"2025-08-14T17:10:49.254690Z","shell.execute_reply.started":"2025-08-14T17:07:55.062361Z","shell.execute_reply":"2025-08-14T17:10:49.253818Z"}},"outputs":[{"name":"stdout","text":"Found 800 images belonging to 10 classes.\nFound 199 images belonging to 10 classes.\n\nTraining Transfer Learning model (VGG16)...\nEpoch 1/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - accuracy: 0.1552 - loss: 2.6833 - val_accuracy: 0.1005 - val_loss: 2.2767 - learning_rate: 1.0000e-05\nEpoch 2/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.3850 - loss: 1.7893 - val_accuracy: 0.1005 - val_loss: 2.1929 - learning_rate: 1.0000e-05\nEpoch 3/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.4988 - loss: 1.5276 - val_accuracy: 0.2010 - val_loss: 2.1220 - learning_rate: 1.0000e-05\nEpoch 4/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - accuracy: 0.5449 - loss: 1.3322 - val_accuracy: 0.2513 - val_loss: 2.0558 - learning_rate: 1.0000e-05\nEpoch 5/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.5870 - loss: 1.2459 - val_accuracy: 0.3970 - val_loss: 1.9980 - learning_rate: 1.0000e-05\nEpoch 6/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - accuracy: 0.6285 - loss: 1.0511 - val_accuracy: 0.4824 - val_loss: 1.9145 - learning_rate: 1.0000e-05\nEpoch 7/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 142ms/step - accuracy: 0.7270 - loss: 0.8936 - val_accuracy: 0.5377 - val_loss: 1.8417 - learning_rate: 1.0000e-05\nEpoch 8/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - accuracy: 0.7331 - loss: 0.8305 - val_accuracy: 0.5528 - val_loss: 1.7771 - learning_rate: 1.0000e-05\nEpoch 9/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.7802 - loss: 0.6962 - val_accuracy: 0.6080 - val_loss: 1.7021 - learning_rate: 1.0000e-05\nEpoch 10/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - accuracy: 0.7990 - loss: 0.6489 - val_accuracy: 0.6131 - val_loss: 1.5929 - learning_rate: 1.0000e-05\nEpoch 11/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.8382 - loss: 0.5763 - val_accuracy: 0.5879 - val_loss: 1.5237 - learning_rate: 1.0000e-05\nEpoch 12/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.8774 - loss: 0.5200 - val_accuracy: 0.6281 - val_loss: 1.4751 - learning_rate: 1.0000e-05\nEpoch 13/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9002 - loss: 0.4285 - val_accuracy: 0.6482 - val_loss: 1.3844 - learning_rate: 1.0000e-05\nEpoch 14/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.8915 - loss: 0.4373 - val_accuracy: 0.6281 - val_loss: 1.3134 - learning_rate: 1.0000e-05\nEpoch 15/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9175 - loss: 0.3753 - val_accuracy: 0.6131 - val_loss: 1.2687 - learning_rate: 1.0000e-05\nEpoch 16/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9188 - loss: 0.3382 - val_accuracy: 0.6482 - val_loss: 1.2212 - learning_rate: 1.0000e-05\nEpoch 17/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9320 - loss: 0.3105 - val_accuracy: 0.6332 - val_loss: 1.1813 - learning_rate: 1.0000e-05\nEpoch 18/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9504 - loss: 0.2881 - val_accuracy: 0.6533 - val_loss: 1.1218 - learning_rate: 1.0000e-05\nEpoch 19/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9587 - loss: 0.2348 - val_accuracy: 0.6281 - val_loss: 1.1328 - learning_rate: 1.0000e-05\nEpoch 20/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9738 - loss: 0.2053 - val_accuracy: 0.6281 - val_loss: 1.1916 - learning_rate: 1.0000e-05\nEpoch 21/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9599 - loss: 0.2282 - val_accuracy: 0.6181 - val_loss: 1.1711 - learning_rate: 1.0000e-05\nEpoch 22/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9714 - loss: 0.1880 - val_accuracy: 0.6482 - val_loss: 1.0943 - learning_rate: 1.0000e-05\nEpoch 23/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9773 - loss: 0.1737 - val_accuracy: 0.6080 - val_loss: 1.1582 - learning_rate: 1.0000e-05\nEpoch 24/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9868 - loss: 0.1492 - val_accuracy: 0.6633 - val_loss: 1.0885 - learning_rate: 1.0000e-05\nEpoch 25/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9883 - loss: 0.1523 - val_accuracy: 0.6633 - val_loss: 1.1157 - learning_rate: 1.0000e-05\nEpoch 26/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9814 - loss: 0.1411 - val_accuracy: 0.6482 - val_loss: 1.1736 - learning_rate: 1.0000e-05\nEpoch 27/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9904 - loss: 0.1163 - val_accuracy: 0.6382 - val_loss: 1.1885 - learning_rate: 1.0000e-05\nEpoch 28/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9981 - loss: 0.0966 - val_accuracy: 0.6482 - val_loss: 1.1735 - learning_rate: 1.0000e-05\nEpoch 29/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9951 - loss: 0.1008 - val_accuracy: 0.6231 - val_loss: 1.2242 - learning_rate: 1.0000e-05\nEpoch 30/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9955 - loss: 0.0943 - val_accuracy: 0.6583 - val_loss: 1.1811 - learning_rate: 5.0000e-06\nEpoch 31/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9982 - loss: 0.0789 - val_accuracy: 0.6533 - val_loss: 1.1882 - learning_rate: 5.0000e-06\nEpoch 32/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9983 - loss: 0.0701 - val_accuracy: 0.6533 - val_loss: 1.1911 - learning_rate: 5.0000e-06\nEpoch 33/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9983 - loss: 0.0731 - val_accuracy: 0.6633 - val_loss: 1.1668 - learning_rate: 5.0000e-06\nEpoch 34/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9944 - loss: 0.0789 - val_accuracy: 0.6533 - val_loss: 1.2149 - learning_rate: 5.0000e-06\nEpoch 35/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9942 - loss: 0.0803 - val_accuracy: 0.6583 - val_loss: 1.1790 - learning_rate: 2.5000e-06\nEpoch 36/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9999 - loss: 0.0592 - val_accuracy: 0.6633 - val_loss: 1.1799 - learning_rate: 2.5000e-06\nEpoch 37/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9975 - loss: 0.0625 - val_accuracy: 0.6583 - val_loss: 1.1996 - learning_rate: 2.5000e-06\nEpoch 38/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9996 - loss: 0.0619 - val_accuracy: 0.6583 - val_loss: 1.2048 - learning_rate: 2.5000e-06\nEpoch 39/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9992 - loss: 0.0679 - val_accuracy: 0.6633 - val_loss: 1.1759 - learning_rate: 2.5000e-06\nEpoch 40/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.0606 - val_accuracy: 0.6683 - val_loss: 1.1736 - learning_rate: 1.2500e-06\nEpoch 41/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 1.0000 - loss: 0.0646 - val_accuracy: 0.6683 - val_loss: 1.1688 - learning_rate: 1.2500e-06\nEpoch 42/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9990 - loss: 0.0598 - val_accuracy: 0.6633 - val_loss: 1.1781 - learning_rate: 1.2500e-06\nEpoch 43/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9954 - loss: 0.0539 - val_accuracy: 0.6633 - val_loss: 1.1838 - learning_rate: 1.2500e-06\nEpoch 44/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9991 - loss: 0.0544 - val_accuracy: 0.6633 - val_loss: 1.1936 - learning_rate: 1.2500e-06\nEpoch 45/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9970 - loss: 0.0642 - val_accuracy: 0.6683 - val_loss: 1.1948 - learning_rate: 6.2500e-07\nEpoch 46/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9954 - loss: 0.0557 - val_accuracy: 0.6683 - val_loss: 1.1925 - learning_rate: 6.2500e-07\nEpoch 47/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.0465 - val_accuracy: 0.6633 - val_loss: 1.1964 - learning_rate: 6.2500e-07\nEpoch 48/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 1.0000 - loss: 0.0610 - val_accuracy: 0.6583 - val_loss: 1.1976 - learning_rate: 6.2500e-07\nEpoch 49/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9972 - loss: 0.0640 - val_accuracy: 0.6683 - val_loss: 1.1880 - learning_rate: 6.2500e-07\nEpoch 50/50\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9986 - loss: 0.0538 - val_accuracy: 0.6683 - val_loss: 1.1866 - learning_rate: 3.1250e-07\n\n--- Model Accuracy Comparison ---\n                             Model  Accuracy\nRandomForest (MFCC+Extra Features)  0.710000\n           VGG16 Transfer Learning  0.668342\n","output_type":"stream"}],"execution_count":20}]}